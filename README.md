# EphemerAl

Looking for a complete solution to host a web-based AI service on your local network? Skip the costs and data risks of online platforms by running Googleâ€™s Gemma 3 LLM on your own hardware. Gemma 3 is the highest-rated open-weight LLM thatâ€™s practical to run on consumer-grade systems: https://lmarena.ai/leaderboard/text

EphemerAl is a lightweight, self-hosted AI interface designed with local data privacy in mind. No chat history is stored and nothing leaves your network. The included Setup Guide walks you through configuring its Ollama/Gemma 3/Tika backend. All responses are generated by Gemma 3 (12b or 27b), and enhanced by any documents, images, or queries you attach during a conversation.

This project began as a personal effort to solve a problem at my workplace. Iâ€™m not a developer by trade, but I used AI tools to help bring it to life. While it wasnâ€™t built for broad distribution, Iâ€™m sharing this generalized version in case it helps others looking for a secure, account-free, multimodal LLM interface. . . whether for internal tools, training environments, or experimentation.

---

## Core Features of the solution:

* **ðŸ¤– AI Assistant:** Real-time chat interface powered by Googleâ€™s Gemma 3 IT QAT LLM, served locally via Ollama
* **ðŸ“„ Document Upload:** Supports 100+ file types through Apache Tika, injecting full text directly into queries
* **ðŸ–¼ï¸ Multimodal Support:** Use Gemma 3â€™s built-in capability to analyze images alongside text
* **ðŸŽ¨ Customizable Interface:** Clean, minimal UI with optional logo or image block for lightweight branding
* **ðŸ”’ Ephemeral** No session data is stored. All processing runs locally. No internet access is required after initial setup

---

## Architecture

EphemerAl is a containerized web app designed to run fully offline after initial setup. It uses Docker Compose and Windows Subsystem for Linux 2 (WSL2) to create a clean, isolated environment on your existing Windows 11 Pro or Enterprise system.

Thereâ€™s no installer. Instead, you follow a detailed step-by-step deployment guide. Every command is provided, and no prior Docker or WSL experience is required.

Once deployed, the system runs entirely on your local hardwareâ€”no cloud connection, no user accounts, and no data persistence unless you build it in.

**Stack Overview:**

* **Windows 11 Pro/Enterprise Host**

  * **WSL2 (Ubuntu 24.04):** Lightweight Linux VM managed by Windows

    * **Docker Engine:** Manages isolated service containers:

      * `ephemeral-app`: Streamlit-based user interface
      * `ollama`: Local LLM backend using Ollama to serve Gemma 3
      * `tika-server`: Apache Tika for document parsing and extraction

---

## System Requirements & Setup

* **OS:** Windows 11 Pro or Enterprise (fully updated)
* **GPU:** At least one discrete NVIDIA GPU

  * 30-series or newer strongly recommended
  * 12GB+ VRAM suggested for smooth performance with Gemma 3 12B
* **Driver:** Latest WHQL NVIDIA GPU driver (you can skip optional installs like the Control Panel)

## Deployment

Follow the step-by-step instructions in the included **System Deployment Guide**. Most commands are ready to copy and paste into PowerShell.
Setup includes optional instructions for launching the app automatically at login (useful for unattended or kiosk-style deployments).

> A fully headless WSL setup was attempted but ultimately abandoned in favor of simplicity and reliability during operation.

## Access the Interface

* **From the server:**
  Visit [http://localhost:8501](http://localhost:8501)

* **From another machine on your network:**
  Replace `<windows_host_ip_address>` with the correct IP:
  `http://<windows_host_ip_address>:8501`

## Stopping the Application

To stop the application, open an elevated (Administrator) PowerShell window on the host and run:

```powershell
wsl --shutdown
```

To restart, either:

* Reboot and log in, or
* Run `wsl` again from PowerShell and leave the window open or minimized

## Support

No official support is provided. If you run into issues, I recommend:

* Pasting the error message into an AI assistant
* Including a screenshot and a brief description of your technical level

This improves the chances of getting helpful troubleshooting advice.

## Known Issues

* The UI doesn't render correctly on mobile.
* Minimizing the sidebar (where the logo lives) can't be undone without a refresh.
* User text may initially appear under the right arrow icon in the input box before wrapping correctly.
* Attachments disappear visually after submission, but their content remains in context for the model throughout the conversation.
* There is no guardrail for exceeding the model's `ctx` limit. Gemma 3 supports very long context windows, and I chose to defer graceful error handling unless this becomes a problem.

