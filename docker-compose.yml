###############################################################################
# docker-compose.yml — EphemerAl demo stack
#
# Three-service layout:
#   • ollama        — serves the LLM (GPU / CUDA)
#   • tika-server   — parses over 100 document types
#   • ephemeral-app — Streamlit front-end website
#
# Security defaults:
#   • UI is exposed on 8501
#   • Ollama (11434) and Tika (9998) are bound to 127.0.0.1 on the host
#     so they are NOT reachable from other LAN devices by default.
#
# Why: Docker-published ports can bypass UFW in common configurations.
###############################################################################

services:
  # --------------------------------------------------------------------------
  # Large-language-model back-end (Ollama)
  # --------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:0.13.5
    container_name: ollama
    restart: unless-stopped
    networks: [llm-net]

    # Model weights live here; bind to a fast disk if possible
    volumes:
      - ollama-models:/root/.ollama

    environment:
      # NVIDIA_VISIBLE_DEVICES=all lets the container see every GPU the host has.
      - NVIDIA_VISIBLE_DEVICES=all

      # Listen on the container network so ephemeral-app can reach it at http://ollama:11434
      - OLLAMA_HOST=0.0.0.0

      # Browser CORS for direct Ollama API access.
      # EphemerAl does not require this (server-side calls).
      # If you intentionally expose Ollama to a browser, set this deliberately.
      # - OLLAMA_ORIGINS=*

      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FORCE_CUBLAS_LT=1
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_FLASH_ATTENTION=1

    # Publish Ollama to the host, but bind to localhost only
    ports:
      - "127.0.0.1:11434:11434"

    # GPU reservation for Docker Compose
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Disable Docker logs for this container (privacy)
    logging:
      driver: "none"

  # --------------------------------------------------------------------------
  # Apache Tika — document parsing
  # --------------------------------------------------------------------------
  tika-server:
    image: apache/tika:3.2.3.0-full
    container_name: tika-server
    restart: unless-stopped
    networks: [llm-net]

    environment:
      - JAVA_TOOL_OPTIONS=-Xmx2g -Xms512m

    # Publish Tika to the host, but bind to localhost only
    ports:
      - "127.0.0.1:9998:9998"

    # Keep /tmp in RAM so uploaded docs never touch disk inside the container
    tmpfs:
      - /tmp

    # Disable Docker logs for this container (privacy)
    logging:
      driver: "none"

  # --------------------------------------------------------------------------
  # Streamlit front-end (EphemerAl)
  # --------------------------------------------------------------------------
  ephemeral-app:
    build: .                              # uses local Dockerfile
    container_name: ephemeral-app
    restart: unless-stopped
    networks: [llm-net]

    # Expose the UI to the LAN
    ports:
      - "0.0.0.0:8501:8501"

    environment:
      - LLM_BASE_URL=http://ollama:11434/v1
      - LLM_MODEL_NAME=gemma3-prod
      - TIKA_URL=http://tika-server:9998
      - TIKA_CLIENT_ONLY=true
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      # - EPHEMERAL_TIMEZONE=America/New_York

    depends_on: [ollama, tika-server]

    # Static assets (logo, CSS) mounted read-only
    volumes:
      - ./static:/app/static:ro

    # Keep /tmp in RAM for any temporary upload buffers
    tmpfs:
      - /tmp

    # Rotated logs for debugging
    logging:
      driver: "local"
      options:
        max-size: "2m"
        max-file: "3"

# --------------------------------------------------------------------------
# Shared network — keeps container DNS simple (ollama, tika-server, etc.)
# --------------------------------------------------------------------------
networks:
  llm-net:
    driver: bridge

# --------------------------------------------------------------------------
# Named volume for Ollama models (survives container rebuilds)
# --------------------------------------------------------------------------
volumes:
  ollama-models: {}
