###############################################################################
# docker-compose.yml — EphemerAl demo stack
#
# Three-service layout:
#   • ollama        — serves the LLM (GPU / CUDA)
#   • tika-server   — parses over 100 document types
#   • ephemeral-app — Streamlit front-end website
#
# CUSTOMIZE comments below flag the lines that most people tweak:
#   • LLM model name (12b vs 27b)
#   • Service / container names if you prefer different labels
###############################################################################

services:
  # --------------------------------------------------------------------------
  # Large-language-model back-end (Ollama)
  # --------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:0.13.5
    container_name: ollama
    restart: unless-stopped
    networks: [llm-net]

    # Model weights live here; bind to a fast disk if possible
    volumes:
      - ollama-models:/root/.ollama

    environment:
      # NVIDIA_VISIBLE_DEVICES=all lets the container see every GPU the host has.
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FORCE_CUBLAS_LT=1
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_FLASH_ATTENTION=1

    # Expose Ollama's API port to the host
    ports: ["11434:11434"]

    # GPU reservation for Docker Compose v3+
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # "count: all" automatically uses all available GPUs detected by Docker.
              count: all
              capabilities: [gpu]

    # Disable Docker logs for this container (privacy)
    logging:
      driver: "none"

  # --------------------------------------------------------------------------
  # Apache Tika — document parsing
  # --------------------------------------------------------------------------
  tika-server:
    image: apache/tika:3.2.3.0-full
    container_name: tika-server
    restart: unless-stopped
    networks: [llm-net]
    environment:
      - JAVA_TOOL_OPTIONS=-Xmx2g -Xms512m
    ports: ["9998:9998"]

    # Keep /tmp in RAM so uploaded docs never touch disk inside the container
    tmpfs:
      - /tmp

    # Disable Docker logs for this container (privacy)
    logging:
      driver: "none"

  # --------------------------------------------------------------------------
  # Streamlit front-end (EphemerAl)
  # --------------------------------------------------------------------------
  ephemeral-app:
    build: .                              # uses local Dockerfile
    container_name: ephemeral-app
    restart: unless-stopped
    networks: [llm-net]

    ports: ["0.0.0.0:8501:8501"]

    environment:
      - LLM_BASE_URL=http://ollama:11434/v1
      - LLM_MODEL_NAME=gemma3-prod         # CUSTOMIZE: Matches the generic name created in deployment guide
      - TIKA_URL=http://tika-server:9998
      - TIKA_CLIENT_ONLY=true
      - STREAMLIT_SERVER_HEADLESS=true
      - STREAMLIT_SERVER_ADDRESS=0.0.0.0
      # - EPHEMERAL_TIMEZONE=America/New_York   # CUSTOMIZE: Override system timezone (optional)

    depends_on: [ollama, tika-server]

    # Static assets (logo, CSS) mounted read-only
    volumes:
      - ./static:/app/static:ro

    # Keep /tmp in RAM for any temporary upload buffers
    tmpfs:
      - /tmp

    # Rotated logs for debugging.
    # For a final kiosk box, you can change driver: "local" → "none".
    logging:
      driver: "local"
      options:
        max-size: "2m"
        max-file: "3"

# --------------------------------------------------------------------------
# Shared network — keeps container DNS simple (ollama, tika-server, etc.)
# --------------------------------------------------------------------------
networks:
  llm-net:
    driver: bridge

# --------------------------------------------------------------------------
# Named volume for Ollama models (survives container rebuilds)
# --------------------------------------------------------------------------
volumes:
  ollama-models: {}
