diff --git a/README.md b/README.md
index a5126d2ad4dfaf4ca3124ab90dd66a66687fdc53..4422ca34b1e3841b1df422285a5590c12c034a68 100644
--- a/README.md
+++ b/README.md
@@ -1,46 +1,46 @@
 # EphemerAl: A Simple Self-Hosted Chat Interface for Local AI with Ollama that Accepts Documents and Images
 
 EphemerAl is a lightweight, open-source web interface for interacting with Google's Gemma 3 LLM locally on your hardware. I designed it for my day job to help keep our team's sensitive info off cloud services, and to provide a modern AI experience to staff without the per-user cost required to achieve equivalent capabilities online. All responses are generated by Gemma 3 (12b or 27b), and enhanced by any documents, images, or queries you attach during a conversation. Gemma 3 4b will work if you're hardware limited and want to try it out, but I found it was more Ai than AI. 
 
 While it wasn't built for broad distribution, I'm sharing this generalized version in case it helps others looking for a local-only, account-free, multimodal LLM interface. . . whether to provide an operational tool, a staff learning environment, or bragging rights when friends visit on your home network.
 
 [View the full source code on GitHub](https://github.com/eowensai/EphemerAl)
 
 ![A screenshot of EphemerAl, a Docker-based self-hosted AI assistant for local LLM document Q&A and image analysis using Ollama](Ephemeral%20Screenshot.jpg)
 
 ---
 
 ## Core Features
 
 This tool offers a straightforward set of capabilities to facilitate interaction with local LLM models.
 
 - **Local AI Interaction:** Engage in real-time conversations powered by Google's Gemma 3 model (12B or 27B variants) through Ollama, supporting tasks such as question answering and idea generation without requiring an internet connection.
 - **Document and Image Uploads:** Submit one or more files (including PDFs, documents, and spreadsheets, handling over 100 formats via Apache Tika), allowing the model to incorporate user content into its responses.
 - **Multimodal Functionality:** Leverage Gemma 3's ability to process images in conjunction with text, enabling analysis of visual elements like diagrams or photographs.
 - **Customizable Interface:** Incorporate a personal logo or adjust the appearance if desired using Streamlit, maintaining a clean and user-friendly design.
-- **Ephemeral Sessions:** Chat content is cleared when you refresh, start a new conversation, or close your browser. No conversation history is stored in a database. Document parsing is cached only for the duration of your session to improve performance on repeated uploads.
+- **Ephemeral Sessions:** Chat content is cleared when you start a new conversation or close your browser. No conversation history is stored in a database. Document parsing is cached only for the duration of your session to improve performance on repeated uploads.
 - **Simplified Deployment:** The solution utilizes Docker Compose for containerized setup, making it accessible even for those new to such environments.
 
 ## Privacy Notes
 
 EphemerAl is designed to minimize data retention:
 
 - **No database:** Conversations live in the Streamlit server process memory (`st.session_state`) for your browser session and are not persisted to disk by this app.
 - **Session-scoped caching:** Document parsing results are cached per-session for performance, but cleared when you start a new conversation or close your browser.
 - **Container isolation:** Tika and the Streamlit app use tmpfs for `/tmp`, so temporary files stay in RAM.
 - **Optional log suppression:** For hardened deployments, container logging can be disabled entirely (see docker-compose.yml comments).
 
 Note that browser caching behavior depends on your browser settings and cache-control headers. For maximum privacy on shared machines, use private/incognito browsing or clear browser data after use.
 
 ## Technical Stack
 
 EphemerAl leverages a straightforward application stack to ensure reliability and ease of use.
 
 - **Frontend Framework:** Python 3.11 with Streamlit for the web-based chat interface.
 - **AI Backend:** Ollama to manage Gemma 3 models, optimized for NVIDIA GPU acceleration.
 - **File Processing:** Apache Tika server for extracting text from uploaded documents.
 - **Containerization:** Docker and Docker Compose for isolated deployment.
 - **Dependencies:** Essential libraries such as requests, pytz, tika, openai client, and pillow.
 
 ## System Requirements
 
